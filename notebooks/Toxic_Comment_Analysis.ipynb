{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a80380-8cf0-4b35-a481-374777c5d394",
   "metadata": {},
   "source": [
    "### 1. Load Dataset\n",
    "\n",
    "This initial step involves importing the `pandas` library, a fundamental tool for data manipulation in Python. We then specify the path to our dataset, `data.csv` (which contains the comments and toxicity labels), located in the `data` directory relative to the notebook's location. The `pd.read_csv()` function reads the data from the specified file into a pandas DataFrame named `df`. Finally, `df.head()` displays the first 5 rows of the DataFrame, allowing for a quick inspection of the columns (like `comment_text` and the toxicity labels) and their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90402750-a09e-415c-81fe-fbf5561884cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../data/data.csv' \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ed547-cb94-4474-8d74-cfa5f0033ee3",
   "metadata": {},
   "source": [
    "### 2. Setup Text Preprocessing with spaCy\n",
    "\n",
    "This block imports the necessary libraries, `spacy` for advanced NLP tasks and `re` for regular expression-based cleaning. It then loads a pre-trained small English language model from spaCy (`en_core_web_sm`), disabling the parser and Named Entity Recognition (NER) components (`disable=[\"parser\", \"ner\"]`) as they are not required for our current task, thus improving efficiency. A confirmation message is printed upon successful loading.\n",
    "\n",
    "The core part defines the `preprocess_text_spacy` function, which encapsulates our text cleaning and normalization pipeline:\n",
    "1.  **Input Validation:** Ensures the input is treated as a string.\n",
    "2.  **Basic Cleaning:** It converts text to lowercase and uses regular expressions (`re.sub`) to remove URLs, user mentions (`@username`), HTML tags, any characters that are not lowercase letters or whitespace, and redundant whitespace.\n",
    "3.  **spaCy Processing:** The cleaned text is processed by the loaded `nlp` object, which performs tasks like tokenization and part-of-speech tagging internally.\n",
    "4.  **Token Filtering & Lemmatization:** The code iterates through the resulting spaCy tokens within the processed `doc`. It selects only those tokens that consist purely of alphabetic characters (`token.is_alpha`) and are *not* identified as common English stopwords (`not token.is_stop`). For these selected tokens, their base or dictionary form (lemma) is retrieved using `token.lemma_`.\n",
    "5.  **Output:** Finally, the selected lemmas are joined back together into a single space-separated string. This normalized text, containing only meaningful word stems/bases, is returned and will be used as input for the feature extraction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d899b886-bc72-4cb4-8950-03b5d2f892b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy 'en_core_web_sm' model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"ner\"])\n",
    "    print(\"spaCy 'en_core_web_sm' model loaded successfully.\")\n",
    "except OSError:\n",
    "    print(\"spaCy model 'en_core_web_sm' not found. Please download it:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    if not nlp:\n",
    "        print(\"Warning: spaCy model not loaded. Performing basic cleaning only.\")\n",
    "    if not isinstance(text, str): text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if nlp:\n",
    "        doc = nlp(text)\n",
    "        processed_tokens = [\n",
    "            token.lemma_ for token in doc if token.is_alpha and not token.is_stop\n",
    "        ]\n",
    "        return ' '.join(processed_tokens)\n",
    "    else:\n",
    "         return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0cb044-b85a-4ca7-b111-c65a57493c7c",
   "metadata": {},
   "source": [
    "### 3. Apply Preprocessing to Dataset\n",
    "\n",
    "This code block applies the `preprocess_text_spacy` function (defined in the previous step) to the entire `comment_text` column of our DataFrame `df`.\n",
    "* The `.fillna('')` method is used first to handle any potential missing values (NaN) in the comment column by replacing them with empty strings, ensuring the preprocessing function doesn't encounter errors.\n",
    "* The `.apply()` method then executes `preprocess_text_spacy` for each comment.\n",
    "* The results of this processing (the cleaned and lemmatized text strings) are stored in a new column named `comment_text_processed`.\n",
    "* Finally, the DataFrame `df` is filtered to keep only the rows where `comment_text_processed` is not an empty string (`!= \"\"`). This step removes any comments that might have become empty after preprocessing (e.g., if they only contained stopwords, URLs, mentions, or symbols that were stripped out), ensuring that only entries with relevant textual content proceed to the feature extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14b1642-eeb9-426f-af46-692f8dd5a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after processing and removing empty: 159434 (removed 137)\n"
     ]
    }
   ],
   "source": [
    "df['comment_text_processed'] = df['comment_text'].fillna('').apply(preprocess_text_spacy)\n",
    "initial_rows = df.shape[0]\n",
    "df = df[df['comment_text_processed'] != \"\"]\n",
    "print(f\"Rows after processing and removing empty: {df.shape[0]} (removed {initial_rows - df.shape[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd539d-1fee-4e28-ba20-1e745b691bd6",
   "metadata": {},
   "source": [
    "### 4. Split Data into Training and Testing Sets\n",
    "\n",
    "Before training the model, we need to split our dataset into two parts: one for training the model and another for evaluating its performance on unseen data.\n",
    "* First, we import the `train_test_split` function from `scikit-learn`.\n",
    "* We define our features `X` as the column containing the preprocessed comment text (`comment_text_processed`) and our target variable `y` as the `toxic` column, which holds the labels (0 for non-toxic, 1 for toxic).\n",
    "* The `train_test_split` function is then used to divide `X` and `y` into training sets (`X_train`, `y_train`) and testing sets (`X_test`, `y_test`).\n",
    "    * `test_size=0.2` allocates 20% of the data to the test set and the remaining 80% to the training set.\n",
    "    * `random_state=42` ensures that the split is identical each time the code runs, allowing for reproducible results.\n",
    "    * `stratify=y` is crucial for classification tasks. It ensures that the proportion of toxic and non-toxic comments in both the training and testing sets mirrors the proportion in the original dataset, preventing skewed evaluation due to random chance in the split, especially important if the classes are imbalanced.\n",
    "* The final print statement confirms the number of samples allocated to the training (`X_train`) and testing (`X_test`) feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c448536a-39fd-4cf6-8f8a-da411cd8d6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: 127547, X_test size: 31887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['comment_text_processed']\n",
    "y = df['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train size: {len(X_train)}, X_test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe88a36-810f-4ae1-a6a7-33437e9ec555",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning using RandomizedSearchCV\n",
    "\n",
    "This step aims to find the optimal hyperparameters for both the TF-IDF vectorizer and the LinearSVC classifier to potentially improve model performance.\n",
    "* We import necessary classes including `Pipeline`, `RandomizedSearchCV`, and distributions from `scipy.stats`.\n",
    "* A `Pipeline` is constructed, chaining the `TfidfVectorizer` and `LinearSVC` estimators. Using a pipeline is crucial for applying cross-validation correctly when tuning both preprocessing and classification steps.\n",
    "* A dictionary `param_distributions` defines the search space, specifying ranges or distributions for key hyperparameters of both the TF-IDF step (e.g., `max_features`, `min_df`, `max_df`) and the LinearSVC classifier (`C`), using the `step_name__parameter_name` syntax.\n",
    "* `RandomizedSearchCV` is initialized with the pipeline, the defined parameter distributions, and configured to try `n_iter=50` random combinations, using 3-fold cross-validation (`cv=3`) and optimizing for the `roc_auc` score. `n_jobs=-1` utilizes all available CPU cores, and `verbose=2` shows detailed progress.\n",
    "* The search is executed by calling `.fit()` on the training data (`X_train`, `y_train`). Note that the raw text `X_train` is passed, as the pipeline handles vectorization internally during the search.\n",
    "* Upon completion, the total time taken, the best set of parameters discovered (`.best_params_`), and the highest mean ROC AUC score achieved during cross-validation (`.best_score_`) are printed.\n",
    "* Finally, the `best_estimator_` attribute, which contains the entire pipeline refitted with the optimal hyperparameters found, is stored in the `best_pipeline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82731ceb-2ef1-46da-85ed-1434f6a96316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning ---\n",
      "Starting RandomizedSearchCV... This may take a while.\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Hyperparameter search finished in 254.27 seconds.\n",
      "\n",
      "Best parameters found by RandomizedSearchCV:\n",
      "{'clf__C': np.float64(0.1052037699531582), 'tfidf__max_df': np.float64(0.9028853284501254), 'tfidf__max_features': 14474, 'tfidf__min_df': 3}\n",
      "\n",
      "Best cross-validation ROC AUC: 0.9651\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "print(\"\\n--- Hyperparameter Tuning ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "        ('clf', LinearSVC(class_weight='balanced', random_state=42, max_iter=5000, dual=False))\n",
    "    ])\n",
    "\n",
    "param_distributions = {\n",
    "    'tfidf__max_features': randint(5000, 25000),\n",
    "    'tfidf__min_df': randint(1, 5),\n",
    "    'tfidf__max_df': uniform(0.85, 0.15),\n",
    "    'clf__C': uniform(0.1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = param_distributions,\n",
    "    n_iter = 50,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state = 42,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "print(\"Starting RandomizedSearchCV... This may take a while.\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Hyperparameter search finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\nBest parameters found by RandomizedSearchCV:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\nBest cross-validation ROC AUC: {random_search.best_score_:.4f}\")\n",
    "\n",
    "best_pipeline = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b94b03-86f4-4a5a-8fd0-77471a98ee85",
   "metadata": {},
   "source": [
    "### 6. Evaluate Best Model on Test Set\n",
    "\n",
    "After identifying the best hyperparameter combination using cross-validation on the training data, this step assesses the final performance of that optimized model (`best_pipeline`) on the held-out test set (`X_test`, `y_test`), which was not used during training or tuning.\n",
    "* The `roc_auc_score` metric is imported from `sklearn.metrics`.\n",
    "* The `.decision_function()` method of the `best_pipeline` is called on the raw test text data (`X_test`). The pipeline automatically applies the necessary preprocessing, TF-IDF transformation with the optimized parameters, and the tuned LinearSVC classifier to generate decision scores for the test set.\n",
    "* The `roc_auc_score` function calculates the final ROC AUC score by comparing the true labels (`y_test`) with the predicted decision scores (`y_decision_scores_best`).\n",
    "* The resulting score, printed out, represents an unbiased evaluation of the tuned model's generalization performance on previously unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58b6470-2c58-44e4-a87e-dd3134cab829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Best Model on Test Set ---\n",
      "Best model ROC AUC on TEST set: 0.9661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
    "y_decision_scores_best = best_pipeline.decision_function(X_test)\n",
    "roc_auc_best = roc_auc_score(y_test, y_decision_scores_best)\n",
    "print(f\"Best model ROC AUC on TEST set: {roc_auc_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931775d-8abe-40bb-b555-f2f78d369c75",
   "metadata": {},
   "source": [
    "### 7. Save the Optimized Pipeline\n",
    "\n",
    "This final code block saves the best-performing pipeline (found through hyperparameter tuning and stored in the `best_pipeline` variable) to disk for later use, such as deploying it in the API.\n",
    "* The `joblib` library (for efficient object serialization) and the `os` module (for interacting with the file system) are imported.\n",
    "* An `output_dir` variable specifies the name of the directory where the pipeline will be saved.\n",
    "* `os.makedirs(output_dir, exist_ok=True)` ensures this directory exists, creating it if necessary without raising an error if it's already there.\n",
    "* `os.path.join` constructs the full, operating-system-independent path to the output file (`best_toxicity_pipeline.joblib`) within the specified directory.\n",
    "* `joblib.dump()` serializes the `best_pipeline` object (containing both the optimized TF-IDF vectorizer and the tuned LinearSVC classifier) and writes it to the designated `.joblib` file.\n",
    "* A confirmation message is printed, indicating the path where the pipeline has been successfully saved. This file now contains the complete, ready-to-use model artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "315a9fc1-19a7-40ee-a071-9ae631bf6c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Optimized Pipeline ---\n",
      "Best pipeline saved to: saved_pipeline_spacy_svm/best_toxicity_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Saving Optimized Pipeline ---\")\n",
    "output_dir = 'saved_pipeline_spacy_svm'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pipeline_path = os.path.join(output_dir, 'best_toxicity_pipeline.joblib')\n",
    "joblib.dump(best_pipeline, pipeline_path)\n",
    "\n",
    "print(f\"Best pipeline saved to: {pipeline_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
